<p><strong>Tiêu chí đánh giá</strong></p>
<p>Mỗi nhiệm vụ sẽ có từng tiêu chí đánh giá riêng, trong đó:</p>
<ul>
<li>F1-score: Dùng để đánh giá độ chính xác của nhiệm vụ phát hiện chữ (text localization).</li>
<li>CER: Dùng để đánh giá độ chính xác của nhiệm vụ nhận diện chữ (text recognition)</li>
</ul>
<p>Kết quả cuối cùng sẽ được tính bằng tổng có trọng số của F1 và CER, cụ thể: <em>total_score = 0.5*F1 + 0.5*(1-CER).</em></p>
<p>Chú thích:</p>
<ul>
<li>F1-score = 2* (Precision * Recall) / (Precision + Recall) với cách tính Precision, Recall, F1-score trong nhiệm vụ phát hiện chữ được tham khảo ở <a href="https://github.com/clovaai/TedEval">liên kết</a> này</li>
<li>CER (Character Error Rate) đại diện cho phần trăm ký tự trong văn bản của tệp grouth truth bị dự đoán <strong>không chính xác.<br /></strong>CER = (S + D + I) / N, trong đó S: số ký tự của prediciton bị sai so với groundtruth, D: số ký tự của prediction bị thiếu so với groudtruth, I: số ký tự của prediction cần thêm vào so với groundtruth, N: số ký tự của ground truth</li>
<li>Ví dụ: grouth truth: <strong>0</strong>320<strong>2</strong>2<strong>A</strong>BCD và prediction: O320Z2BCD có hai kí tự cần thay là: O thay thế 0 và Z thay thế 2, một kí tự bị thiếu là: A bị thiếu, không có kí tự nào cần thêm. Do đó S=2, D=1, I=0, CER = (2+1+0)/10= 0.3.</li>
</ul>